<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Ubuntu Problems Summary]]></title>
    <url>%2F2020%2F02%2F13%2FUbuntu-Problems-Summary%2F</url>
    <content type="text"><![CDATA[Ubuntu 登陆时，循环登陆界面常见解决方式如下 重装驱动 在user目录下.xsession-errors 文件中查看错误信息1vim .xsession-errors 如果出现12openConnection: connect: 没有那个文件或目录 cannot connect to brltty at :0 基本就是驱动问题, 卸载驱动重新安装即可 重新安装驱动 进入字符界面 1ctrl + alt + F1 ~ F4 # F7返回图形界面 关闭图形界面 1sudo service lightdm stop 卸载原有驱动 123sudo apt-get remove --purge nvidia-*sudo apt-get autoremove #optional，我没有用也行sudo ./NVIDIA-Linux-x86_64-xxx.run --uninstall #用.run文件安装，可以在该文件所在的文件下卸载 重新安装 1sudo ./NVIDIA-Linux-x86_64-xxx.run --no-opengl-files 打开图形界面 1sudo service lightdm start 改变.Xauthority文件权限 查看用户目录下.Xauthority文件权限1ls -la .Xauthority 正常情况下应该为: -rw———- 1 user user 80 time .Xauthority其中 user 为本电脑的使用者用户名。若为root，则是错误的，进行步骤2. 将.Xauthority的拥有者改为登陆用户1sudo chmod user:user .Xauthority 改变tmp目录权限 进入~/.xsession-errors文件，若是提示：“mkdtemp: private socket dir:Permission denied”便是tmp权限问题1chmod 1777 /tmp]]></content>
  </entry>
  <entry>
    <title><![CDATA[BERT]]></title>
    <url>%2F2020%2F02%2F10%2FBERT%2F</url>
    <content type="text"><![CDATA[a brief introduction to BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Abstact BERT: B idirectional E ncoder R epresentations from T ransformers Highlight: Bidirectional (use both left and right context in all layers) Model: pre-training + fine-tuning Introduction why Bidirectional:standard language models are unidirectional -&gt; limits the choice of architectures that can be used during pre-training &amp;&amp; restrict the power of the pre-trained representations two strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. Feature-based approach: using task-specific architectures that include the pre-trained representations as additional features. (eg. ELMo) Fine-tuning approach: simply fine-tuning all pretrained parameters when trained on the downstream tasks. can introduce minimal task-specific parameters. (eg. OpenAI GPT)both of them share the same objective function during pre-training. BERT pretraining tasks: MLM(masked language model) + NSP(next sentence prediction) BERT code ApproachOverall Architectures pre-training + fine-tuning: pre-training: trained on unlabeled data. fine-tuning: first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Multi-layer bidirectional Transformer encoder (based on tensor to tensor) Notation: the number of layers(i.e., Transformer blocks): L the hidden size: H the number of self-attention heads: A two model sizes: BERTBASE: L=12, H=768, A=12, Total Parameters=110M BERTLARGE: L=24, H=1024, A=16, Total Parameters=340M Input/Output Representations Concept: sentence : an arbitrary span of contiguous text, rather than an actual linguistic sentence. sequence : the input token sequence to BERT, which may be a single sentence or two sentences packed together. How to process text: using WordPiece embeddings with a 30,000 token vocabulary. using [CLS] as the first token of every sequence. using [SEP] to sepetate different sentences in a sequence. / adding a learned embedding to every token to indicate whether it belongs to sentence A or sentence B. Final Input Representation = token embeddings + segment embeddings + position embeddings Pre-trainingMasked Language Model (MLM) the problem when using bidirectional model: allow each word to indirectly ‘see itself’solution: using MLM the downside of a bidirectional pre-trained model: a mismatch between pre-training and fine-tuningsolution: taking different strategies to replace masked token Disign: mask 15% of all WordPiece tokens in each sequence at random. replace the mask token in 3 ways: (1) 80%: the [MASK] token. (2) 10%: a random token. (3) 10%: unchanged. the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary Loss function: cross-entropy(predict token, ground-truth) Next Sentence Prediction (NSP) Input: negative pairs : positive pairs = 1 : 1 C: the final hidden vectors of [CLS] -&gt; binary classification -&gt; IsNext / NotNext(PS. The vector C is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.) Fine-tuning Input: a sequence(sentence A and sentence B)for different tasks, they are: sentence pairs in paraphrasing, hypothesis-premise pairs in entailment question-passage pairs in question answering a degenerate text-$\emptyset$ pair in text classification or sequence tagging Output: token representations are fed into an output layer for token-level tasks (eg. sequence tagging or question answering) the [CLS] representation is fed into an output layer for classification (eg. entailment or sentiment analysis)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Unicoder-VL]]></title>
    <url>%2F2020%2F02%2F09%2FUnicoder-VL%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Basic knowledge of Mathematics]]></title>
    <url>%2F2020%2F01%2F26%2FBasic-knowledge-of-Mathematics%2F</url>
    <content type="text"><![CDATA[总结了常用的数学常识，方便自己查阅。 微积分微积分包括微分和积分，积分包括不定积分和定积分。 微分 ≈ 求导 积分，求导的逆过程。不定积分：求一个函数f(x)的不定积分,就是要求出一个原函数F(x)。定积分：定积分与不定积分的区别是,定积分有上下限。 因而不定积分的结果往往是个函数,定积分的结果则是个常数。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog]]></title>
    <url>%2F2019%2F12%2F29%2FMulti-step-Reasoning-via-Recurrent-Dual-Attention-for-Visual-Dialog%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[VSRN: Visual Semantic Reasoning for Image-Text Matching]]></title>
    <url>%2F2019%2F10%2F21%2FVSRN-Visual-Semantic-Reasoning-for-Image-Text-Matching%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Bidirectional Retrieval Made Simple]]></title>
    <url>%2F2019%2F07%2F25%2FBidirectional-Retrieval-Made-Simple%2F</url>
    <content type="text"><![CDATA[a brief introduction to Bidirectional Retrieval Made Simple —CHAIN-VSE 0.Abstract (1) put words, phrases, sentences, and images into a jointly shared space (2) Loss func: same to Order Embedding[1] 1.CHAIN-VSE inception-based modules named Character-level Inception for Visual-Semantic Embeddings (1) V1 Consists of two inception modules a. First module: maps the binary character-level(one-hot) content from the sentences -&gt; dense representation by convolving the characters with ﬁlter sizes f ∈ {7,5,3}, generating word-embedding-like vectors. b. Second module: consists in using four independent convolutional streams 2.Experiment(1)Hyper-Parameters: p: regulates the number of ﬁlters in the convolutional layers d: deﬁnes the latent-embedding size [1] Order-Embeddings of images and language]]></content>
  </entry>
  <entry>
    <title><![CDATA[YFCC100M]]></title>
    <url>%2F2019%2F07%2F19%2FYFCC100M%2F</url>
    <content type="text"><![CDATA[a brief introduction to YFCC100MYFCC100M: The New Data in Multimedia Research 1.Download:(1) https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&amp;did=67&amp;guccounter=1 click the button selcect this dataset to add I3 into your request(2) requisite: Yahoo account, AWS account (PS: when you register an AWS account, you need to input your credit/debit card ID. But it won’t deduct your money.)(3) When applying the I3, you are supposed to fill in the AWS identifier, which is a 64-bit string. (where to get it: click the website provided in the ‘?’. -&gt; click the last line which is account information)(4) if you apply successfully, you will receive a mail.(5) when you are informed to download the dataset(in a mail), you can follow the instructions offered in the mail.(6) Notice: ‘aws configure’ -&gt; you will need to input your actionkey, secrect key, default region, and default output.about the region: you should input the code provided in the page where you get your actionkey(IAM HOME).(eg: us_west_2 -&gt; Oregon; ap_northeast_1 -&gt; Tokyo) 2.Metadata Line number Photo/video identifier Photo/video hash User NSID User nickname Date taken Date uploaded Capture device Title Description User tags (comma-separated) Machine tags (comma-separated) Longitude Latitude Accuracy of the longitude and latitude coordinates (1=world level accuracy, …, 16=street level accuracy) Photo/video page URL Photo/video download URL License name License URL Photo/video server identifier Photo/video farm identifier Photo/video secret Photo/video secret original Extension of the original photo Photos/video marker (0 = photo, 1 = video)]]></content>
  </entry>
  <entry>
    <title><![CDATA[X-path tips]]></title>
    <url>%2F2019%2F06%2F30%2FX-path-tips%2F</url>
    <content type="text"><![CDATA[a brief summarization about the usage of X-path that I often used and problems I met. search elementfind_element_by_idfind_element_by_namefind_element_by_xpathfind_element_by_link_textfind_element_by_partial_link_textfind_element_by_tag_namefind_element_by_class_namefind_element_by_css_selectorps: change ‘element’ to ‘elements’ -&gt; return a list of all elements found the [] begin with 1 (not 0)[n] &lt;==&gt; position() = n /child::xx &lt;==&gt; /xx/child::* : find all child nodes Problem:Unable to locate: “No Such Element Exception: Unable to locate element”Solution:(1) check path(2) if the path is right, the possible reason is that the element is not fully loaded.a. use While(): break when the element is loaded.b. use WebDriverWait(driver, time).until(element):eg: from selenium.webdriver.support.ui import WebDriverWait wait = ui.WebDriverWait(driver,10) wait.until(lambda driver: driver.find_element_by_xpath(“path”)) OR element = WebDriverWait(driver, 10).until( EC.presence_of_element_located((By.CLASS_NAME, “search-show”))) element.click()]]></content>
  </entry>
  <entry>
    <title><![CDATA[Pytorch Moudule]]></title>
    <url>%2F2019%2F06%2F24%2FPytorch-Moudule%2F</url>
    <content type="text"><![CDATA[Containers: torch.nn.moduleimport torch.nn as nnimport torch.nn.functional as F Core Function(1) add_module(name,module): add child module to current module; can be obtained by its name;Eg: self.add_module(“conv”, nn.Conv2d(10, 20, 4)) can be replaced by -&gt; self.conv = nn.Conv2d(10, 20, 4)(2) forward(*input): should be rewrited in user-difined module;(3) state_dict(destination=None): return a dictionary; save all the states of the module;(4) load_state_dict(state_dict):load the parameters of the module 2.(1) parameters(memo=None):return an iterator involving all parameters in the module; often used as the parameter forFunction Optimizer;(2) children():(3) name_children():(4) modules():(5) named_modules(): 3.change parameters &amp; buffers to XX(1) cpu(device_id=None)：(2) cuda(device_id=None)：(3) double():(4) float()：(5) half()： Setting(1) train(mode=True)：change the module to trainging mode; only inflence dropout &amp; batchNorm;(2) eval(): change to module to evaluation mode; only inflence dropout &amp; batchNorm;(3) zero_grad(): set all grads in the module as 0; Register(1) register_parameter(name, param): add parameter to module; the param can be obtained by name;(2) register_forward_hook(hook)：register a forward hook to module; the hook will be used every time the forward() is used; (3) register_backward_hook(hook)：(4) register_buffer(name, tensor)：add a persistent buffer to module; often used to save a state which doesn’t need to regard as module params; torch.nn.Sequential(*args) add layers to ContainersEg: # Example of using Sequential model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Example of using Sequential with OrderedDict model = nn.Sequential(OrderedDict([ (&#39;conv1&#39;, nn.Conv2d(1,20,5)), (&#39;relu1&#39;, nn.ReLU()), (&#39;conv2&#39;, nn.Conv2d(20,64,5)), (&#39;relu2&#39;, nn.ReLU()) ])) Reference: https://zhuanlan.zhihu.com/p/34616199]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu software install]]></title>
    <url>%2F2019%2F05%2F28%2FUbuntu-software-install%2F</url>
    <content type="text"><![CDATA[Anaconda3 + Cuda9.0 + Cudnn7.1.4 + Pytorch1.1 + PycocotoolsAnaconda3 Home: https://www.anaconda.com/ Linux Python3.7 Version -&gt; bash .run File use ‘conda list’ to verify if install successfull Cuda9.0 Home: https://developer.nvidia.com/cuda-toolkit-archive Select cuda9.0 Linux | x86_64 | Ubuntu | 16.04 | run Run ‘sudo sh cuda_9.0*.run’(follow the steps in website) While installation: choose ‘no’ to the second question(refuse to install GPU driver) Mistake solution: (1)msg:’Missing recommended library: libGLU.so Missing recommended library: libX11.so Missing recommended library: libXi.so Missing recommended library: libXmu.so’ solution: sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev Environment Variables: sudo vi ~/.bashrc #add following at the end of the file export PATH=/usr/local/cuda-9.0/bin:$PATH export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64 #注意冒号 #update configuration sudo source ~/.bashrc Verify if install successfull: cd /usr/local/cuda-9.0/samples/1_Utilities/deviceQuery sudo make ./deviceQuery #if succeed, the imformation about GPU will be show on the terminal Check the version: cat /usr/local/cuda/version.txt Cudnn 7.1.4 Home: https://developer.nvidia.com/rdp/cudnn-archive choose ‘7.1.4 for Linux’ download the .tgz File Cmd: tar -zxvf cudnn-9.0-linux-x64-v7.1.tgz cd cuda sudo cp lib64/lib /usr/local/cuda-9.0/lib64/ sudo cp include/cudnn.h /usr/local/cuda-9.0/include/ sudo cp cuda/lib64/libcudnn /usr/local/cuda-9.0/lib64 sudo chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn* #update the link cd /usr/local/cuda-9.0/lib64/ sudo chmod +r libcudnn.so.7.1.3 #use &#39;ls libcud*&#39; -&gt; comfirm the version of .so sudo ln -sf libcudnn.so.7.1.3. libcudnn.so.7 sudo ln -sf libcudnn.so.7 libcudnn.so sudo ldconfig Check the Version: 1cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 Pytorch 1.1 Home: https://pytorch.org/get-started/locally/ Select the related version use the cmd provided by the web Pycocotools1234git clone https://github.com/cocodataset/cocoapi.gitcd cocoapi/PythonAPImakepip install pycocotools Reference: https://zhuanlan.zhihu.com/p/44183691 Update nvidia version search and download required driver hereps. GTX 1080Ti is belong to Geforce 10 Series uninstall the original driver 1sudo apt-get remove --purge nvidia* install the new driver 123sudo service lightdm stop #close the GUIsudo ./NVIDIA-Linux-x86_64-390.77.runsudo service lightdm start ps. some options: continue? yes -&gt; DKMS? no -&gt; 32-bit? no -&gt; nvidia-xconfigutilit? yes Reference: https://blog.csdn.net/u013832707/article/details/93157805]]></content>
  </entry>
  <entry>
    <title><![CDATA[Learning Visual N-Grams from Web Data]]></title>
    <url>%2F2019%2F05%2F27%2FLearning-Visual-N-Grams-from-Web-Data%2F</url>
    <content type="text"><![CDATA[a brief introduction to Learning Visual N-Grams from Web Data Summary(1) Dataset: YFCC100M (images-user comments pairs)[30 million](2) Image Feature: ResNet34(3) Loss function: Naive n-gram loss(multi-class logistic loss) &amp;&amp; Jelinek-Mercer (J-M) loss(Jelinek-Mercer smoothing)(4) Experiment setting: SGD | batch-size:128 | epoch: 10 |learning rate: 0.1[learning rate is divided by 10 whenever the training loss stabilizes (until a minimum learning rate of 0.001)](5) Relating Images and Captions: Image retrieval: rank images according to their log-likelihood for a particular caption Caption retrieval: ranking all captions in the test set according to their log-likelihood under the model. Introduction(1) Visual n-gram models: can predict arbitrary phrases that are relevant to the content of an image.(2) Model Structure: feed-forward convolutional networks + new loss functions(inspired by n-gram models commonly used in language modeling)(3) Follow Learning visual features from large weakly supervised data(4) Bridge the semantic gap between vision and language: predict phrases that are relevant to the contents of an image(5) Loss function: optimizes trainable parameters for frequent n-grams, whereas for infrequent n-grams, the loss is dominated by the predicted likelihood of smaller “sub-grams” Learning Visual N-Gram Models3.1 Dataset(1) YFCC100M: 99.2 million images + associated multi-lingual user comments(2) Utilization: 30 million images and English comments(3) Image pretreatment: rescale to 256 X 256 pixels (using bicubic interpolation) -&gt; crop the central 224 x 224 -&gt; subtract the mean pixel value of each image -&gt; divide by the standard deviation of the pixel values.(4) Text pretreatment: remove punctuations -&gt; add [BEGIN] and [END] tokens at the beginning and end of each sentence(5) Building dictionary of all English n-grams(n: 1 ~ 5): more than 1, 000 occurrences in the 30 million English comments. -&gt; 142, 806 n-grams: 22, 869 unigrams, 56, 830 bigrams, 32, 560 trigrams, 17, 351 four-grams, and 13, 196 five-grams 3.2 Loss functions3.2.1 Naive n-gram loss (1) measures the (negative) log-likelihood of in-dictionary n-grams that are present in a comment (2) multi-class logistic loss (3) the loss: the sum of all n-grams that appear in the sentence w &amp;&amp; ignore n-grams that do not appear in the dictionary (4) Problem: cannot do language modeling -&gt; Reason: it does not model a conditional probability-&gt; Solution: an ad-hoc conditional distribution based on the scores produced by the model at prediction time using a “stupid” back-off model (5) Drawbacks: 1) it ignores out-of-dictionary n-grams entirely during training 2) the parameters E that correspond to infrequent in-dictionary words are difficult to pin down 3.2.2 Jelinek-Mercer (J-M) loss Experiments4.3 Relating Images and Captions]]></content>
  </entry>
  <entry>
    <title><![CDATA[Multimodal Convolutional Neural Networks for Matching Image and Sentence]]></title>
    <url>%2F2019%2F05%2F22%2FMultimodal-Convolutional-Neural-Networks-for-Matching-Image-and-Sentence%2F</url>
    <content type="text"><![CDATA[a brief introduction to Multimodal Convolutional Neural Networksfor Matching Image and Sentence Summary(1) m-CNN provides an end-to-end framework with convolutional architectures toexploit image representation, word composition, and the matching relationsbetween the two modalities.(2) Component: image CNN &amp; matching CNN &amp; MLP]]></content>
  </entry>
  <entry>
    <title><![CDATA[Learning Two-Branch Neural Networks for Image-Text Matching Tasks]]></title>
    <url>%2F2019%2F05%2F22%2FLearning-Two-Branch-Neural-Networksfor-Image-Text-Matching-Tasks%2F</url>
    <content type="text"><![CDATA[a brief introduction to Learning Two-Branch Neural Networksfor Image-Text Matching Tasks Summary(1) According to different ways to learn the similarity score, this paperpropose two variants of two-branch networks that follow these two strategies.(2) 2-branch: extract image and text features respectively.(3) Embedding network: map images and text -&gt; explicit joint embedding space(4) Similarity Network: frame image/text correspondence as a binary classification problem.(given a pair, the goal is to output the probability that the two items match.) introduction(1) Tasks: phrase localization &amp; bi-directional image-sentence retrieval(2) Network architecture:extract image/text features -&gt; fuse them -&gt; similarity score(3) Embedding network: a. Goal: map image and text features b. Each branch: data -&gt; nonlinearities(2 layers) -&gt; L2 normalization c. Loss function: bi-directional ranking loss + neighborhood-preserving constraints (4) Similarity network: a.Structure: nonlinearities(2 layers) -&gt; element-wise product-&gt; single vector -&gt; fully connected layers b. Loss function: logistic regression loss c. Weakness(compared to Embedding net): it no longer has an explicitembedding space and cannot encode structural constraints. Embedding and Similarity networks3.1 Overview of Image-Text Tasksembedding network is suitable for image-text retrieval3.2 Embedding networkSGD + Bi-directional ranking loss Neighborhood-preserving constraint + Neighborhood sampling]]></content>
  </entry>
  <entry>
    <title><![CDATA[CM-GANs: Cross-modal Generative Adversarial Networks for Common Representation Learning]]></title>
    <url>%2F2019%2F05%2F20%2FCM-GANs-Cross-modal-Generative-Adversarial-Networksfor-Common-Representation-Learning%2F</url>
    <content type="text"><![CDATA[a brief introduction to CM-GANs: Cross-modal Generative Adversarial Networksfor Common Representation Learning 0.Summary 0.1 Structure (1) Generative Model: VGG19(image) | word CNN(text) -&gt; fully connected layer(first) + batch normalization + RELU -&gt; fully connected layer(second) + batch normalization + RELU + weight-sharing between 2 paths -&gt; Common representations -&gt; softmax layer &amp; decoder Decoder: fully connected layer(first) + batch normalization + RELU -&gt; fully connected layer(second) -&gt; text | image reconstructions (2) Intra-modality Discriminative Model: reconstructions -&gt; fully connected layer -&gt; sigmoid layer original representation: 1 | reconstructed representation: 0 (3) Inter-modality Discriminative Model: a. two-pathway network b. input: common representation + original representation c. each path: softmax layer -&gt; fully connected layer(first) + batch normalization + RELU -&gt; fully connected layer(second) (out: single-value predicted score) -&gt; sigmoid layer d. Label: Image: image common representation: 1 corresponding text representation &amp; mismatched image common representation: 0 Text: text common representation: 1 corresponding image representation &amp; mismatched text common representation: 0 0.2]]></content>
  </entry>
  <entry>
    <title><![CDATA[Stacked Cross Attention for Image-Text Matching]]></title>
    <url>%2F2019%2F05%2F20%2FStacked-Cross-Attention-forImage-Text-Matching%2F</url>
    <content type="text"><![CDATA[a brief introduction to Stacked Cross Attention for Image-Text Matching 0.Summary (1) Image representation: bottom-up attention + Faster-RNN + ResNet101 (2) Sentence represetation: one-hot vector (word -&gt; 300-dimentional vector) =&gt; bi-directional GRU(AVG forward &amp; backword) (3) Common representation: SCA model a. Input: a set of image features(image feature -&gt; a region in an image) + a set of word features (word feature -&gt; a word in a sentence) b. Output: similarity score c. Component: (Image-Text &amp; Text-Image) Stacked Cross Attention (4) Loss function: hinge-based triplet ranking loss(like vsepp) -&gt; finding the hardest negatives based on the similarity (5) Experiments: a.Datasets: COCO | Flickr 30k]]></content>
  </entry>
  <entry>
    <title><![CDATA[Order-Embeddings of Images and Language]]></title>
    <url>%2F2019%2F05%2F17%2FOrder-Embeddings-of-Images-and-Language%2F</url>
    <content type="text"><![CDATA[a brief introduction to Order-Embeddings of Images and Language Abstact(1) Goal: explicitly model -&gt; the partial order structure of this visual-semantic hierarchy(2) method: learn ordered representations Introduction]]></content>
  </entry>
  <entry>
    <title><![CDATA[VSE++: Improving Visual-Semantic Embeddings with Hard Negatives]]></title>
    <url>%2F2019%2F05%2F15%2FVSE-Improving-Visual-Semantic-Embeddings-with-Hard-Negatives%2F</url>
    <content type="text"><![CDATA[a brief introduction to VSE++: Improving Visual-Semantic Embeddings with Hard Negatives Abstract(1) inspired by: hard negative mining(2) propose: a new loss function (a simple change to common loss function)(3) combined with: fine tune + augmented data Introduction(1) basic problem: one of ranking(2) augmented data + fine tune =&gt; improve Caption retrieval(3) the benefit of a more powerful image encoder, with finetuning =&gt; is amplified with the use of the loss function Learning Visual-Semantic Embeddings(1) the target of I-T retrieval: ↑ R@k(2) the Similarity funtion -&gt; is defined on the joint embedding space3.1 Visual-Semantic Embedding (1) Image Encoder: VGG19 | ResNet152 (2) Word Encoder: GRU-based text encoder (3) mappings into the joint embedding space -&gt; linear projections (4) similarity function: inner product (5) cumulative loss over training data: (6) loss function for a single training exemplar: hinge-based triplet ranking loss3.2 Emphasis on Hard Negatives (1) hard negatives: the negatives closest to each training query. (2) new loss function: (to emphasis the hardest negatives) (3) how to find hardest negatives: find them within each mini-batch + random sampling of the mini-batches3.3 Probability of Sampling the Hardest Negative (1) for large enough mini-batchs, with high probability we sample negative captions that are harder than 90% of the entire training set (2) Mini-batches size: 128 Experiments(1) Image: a. previous work: extract image features directly from FC7, the penultimate fully connected layer dimensionality: 4096 for VGG19 | 2048 for ResNet152 b. steps: resize the image to 256 X 256 use either a single crop of size 224 X 224 or the mean of feature vectors for multiple crops of similar size c. 1C | 10C | RC 1C: one center crop 10C: training with 10 crops at fixed locations RC: a single random crop (used in this paper)(2) Caption: a. dimensionality: 300 -&gt; GRU -&gt; 1024(3) Normalization: a. VSE: the caption embedding is normalized, while the image embedding is not. b. VSEPP: normalize both c. reason: Not normalizing the image embedding changes the importance of samples. -&gt; not normalizing the image embedding helped the VSE0 to find a better solution. However, VSE++ is not significantly affected by this normalization. 3.2 Details of Training (1) Adam optimizer (2) at most 30 epochs a. Common: start -&gt; learning rate **0:0002** for 15 epochs -&gt; learning rate to **0:00002** for another 15 epochs b. fine-tuned: trained for 30 epochs with a fixed image encoder -&gt; then learning rate to **0:00002** for another 15 epochs (3) margin: 0.2 (4) solution to over-fitting: choosing the snapshot of the model that performs best on the validation set The best snapshot: selected based on the sum of the recalls on the validation set 3.5 Improving Order Embeddings training set: 10C + RV (1) Order: copied from original paper *Order-Embeddings of Images and Language* (2)Order0: use training settings in orginal paper. a. start training with a learning rate of **0:001** for 15 epochs b. lower the learning rate to **0:0001** for another 15 epochs c. margin: 0:05 d. takes the absolute value of embeddings before computing the similarity function (3)Order++: use the same learning schedule and margin as the experiments in this paper + MH loss function 3.6 Behavior of Loss Functions MH loss depends on a smaller set of triplets compared to the SH loss]]></content>
  </entry>
  <entry>
    <title><![CDATA[Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval]]></title>
    <url>%2F2019%2F04%2F06%2FWebly-Supervised-Joint-Embedding-for-Cross-Modal-Image-Text-Retrieval%2F</url>
    <content type="text"><![CDATA[A brief introduction to the paper Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval Introduction1.1 Cross-modal Retrieval (1) Task: take one type of data as the query to retrieve relevant data of another type. (2) Challenge: how to measure the content similarity between different modalities of data, which is referred as the heterogeneity gap (3)Common Framework: Feature extraction ——— Correlation modal common representation ——— search result ranking and summarization 1.2 Overview of the Proposed Approach (1) VSE (2) a two-stage approach: A. a supervised formulation —&gt; that leverages the available clean image-text pairs from a dataset an aligned representation --&gt; that can be shared across three modalities (e.g., image, tag, text). loss function --&gt; bi-directional ranking loss B. LUPI multitask learning strategies curriculum guided training strategy Related Work2.1 VSE(Visual-Semantic Embedding) (1)2.2 Image-Text Retrieval (1) Methods: CCA | ACMR | VSE | …… (2) Improving aspects: loss function | similarity calculation | Input feartures2.3 Webly supervised learning Approach3.1 Network Structure and Input Feature (1) Network Structure: A. three different branches: expert network + two fully connected embedding layers expert networks —&gt; focus on identifying modality-specific features embedding layers —&gt; convert the modality specific features to modality-robust features (2) Input Feature A. Image: Resnet152 | VGG19 B. Text(Sentence): Word2Vec + GRU C. Tag: Word2Vec (sum / num) 3.2 Train Joint Embedding with Ranking Loss (1) Method: VSE (2) Representation: i’: image feature i : the projection of image feature on the joint space | i = W(i)i’ s’: text embedding s : the projection of text embedding on the joint space | s = W(s)s’ (3) Loss function: (for VSE) | (for VSE++) s-: non-matching text embedding for image embedding i s : matching text embedding for image embedding i (i &amp; i- are similar to s &amp; s-) ∆ : margin value f(i,s)； scoring function， measuring the similarity between the images and text in the joint embedded space.(cosine similarity used in this work)3.3 Training Joint Embedding with Web Data (1) Obstacle: A. GRU based approach is not suitable for representing tags since tags do not have any semantic context as in the sentences. -&gt; it is not possible to directly update the embedding using image-tag pairs. B. training data do not provide three modality information at the same time -&gt; employing LUPI strategies are also not possible (2) Two-stage approach 1) Training initial Joint Embedding A. consider nouns and verbs from relevant sentence as dummy tags for an image B. combine the image-text ranking loss objective with image-tag ranking loss objective. 2) Model Adaptation with Web Data A. a smaller learning rate B. a curriculum learning-based strategy: in the early stages of training, the network is presented with images related to frequently occurring concepts/keywords in the clean training set. Datasets and Evaluation Metric4.1 Datasets: MSCOCO | Flickr30K | Web Image Collection]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
