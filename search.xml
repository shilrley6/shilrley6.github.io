<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[X-path tips]]></title>
    <url>%2F2019%2F06%2F30%2FX-path-tips%2F</url>
    <content type="text"><![CDATA[a brief summarization about the usage of X-path that I often used and problems I met. search elementfind_element_by_idfind_element_by_namefind_element_by_xpathfind_element_by_link_textfind_element_by_partial_link_textfind_element_by_tag_namefind_element_by_class_namefind_element_by_css_selectorps: change ‘element’ to ‘elements’ -&gt; return a list of all elements found the [] begin with 1 (not 0)[n] &lt;==&gt; position() = n /child::xx &lt;==&gt; /xx/child::* : find all child nodes Problem:Unable to locate: “No Such Element Exception: Unable to locate element”Solution:(1) check path(2) if the path is right, the possible reason is that the element is not fully loaded.-&gt; a. use While(): break when the element is loaded.b. use WebDriverWait(driver, time).until(element):eg: from selenium.webdriver.support.ui import WebDriverWait wait = ui.WebDriverWait(driver,10) wait.until(lambda driver: driver.find_element_by_xpath(“path”)) OR element = WebDriverWait(driver, 10).until( EC.presence_of_element_located((By.CLASS_NAME, “search-show”))) element.click()]]></content>
  </entry>
  <entry>
    <title><![CDATA[Pytorch Moudule]]></title>
    <url>%2F2019%2F06%2F24%2FPytorch-Moudule%2F</url>
    <content type="text"><![CDATA[Containers: torch.nn.moduleimport torch.nn as nnimport torch.nn.functional as F Core Function(1) add_module(name,module): add child module to current module; can be obtained by its name;Eg: self.add_module(“conv”, nn.Conv2d(10, 20, 4)) can be replaced by -&gt; self.conv = nn.Conv2d(10, 20, 4)(2) forward(*input): should be rewrited in user-difined module;(3) state_dict(destination=None): return a dictionary; save all the states of the module;(4) load_state_dict(state_dict):load the parameters of the module 2.(1) parameters(memo=None):return an iterator involving all parameters in the module; often used as the parameter forFunction Optimizer;(2) children():(3) name_children():(4) modules():(5) named_modules(): 3.change parameters &amp; buffers to XX(1) cpu(device_id=None)：(2) cuda(device_id=None)：(3) double():(4) float()：(5) half()： Setting(1) train(mode=True)：change the module to trainging mode; only inflence dropout &amp; batchNorm;(2) eval(): change to module to evaluation mode; only inflence dropout &amp; batchNorm;(3) zero_grad(): set all grads in the module as 0; Register(1) register_parameter(name, param): add parameter to module; the param can be obtained by name;(2) register_forward_hook(hook)：register a forward hook to module; the hook will be used every time the forward() is used; (3) register_backward_hook(hook)：(4) register_buffer(name, tensor)：add a persistent buffer to module; often used to save a state which doesn’t need to regard as module params; torch.nn.Sequential(*args) add layers to ContainersEg: # Example of using Sequential model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Example of using Sequential with OrderedDict model = nn.Sequential(OrderedDict([ (&#39;conv1&#39;, nn.Conv2d(1,20,5)), (&#39;relu1&#39;, nn.ReLU()), (&#39;conv2&#39;, nn.Conv2d(20,64,5)), (&#39;relu2&#39;, nn.ReLU()) ])) Reference: https://zhuanlan.zhihu.com/p/34616199]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu software install]]></title>
    <url>%2F2019%2F05%2F28%2FUbuntu-software-install%2F</url>
    <content type="text"><![CDATA[Anaconda3 + Cuda9.0 + Cudnn7.1.4 + Pytorch1.1 + Pycocotools Anaconda3Home: https://www.anaconda.com/Linux Python3.7 Version -&gt; bash .run Fileuse ‘conda list’ to verify if install successfull Cuda9.0Home: https://developer.nvidia.com/cuda-toolkit-archiveSelect cuda9.0 Linux | x86_64 | Ubuntu | 16.04 | runRun ‘sudo sh cuda_9.0*.run’(follow the steps in website)While installation: choose ‘no’ to the second question(refuse to install GPU driver)Mistake solution: (1)msg:’Missing recommended library: libGLU.so Missing recommended library: libX11.so Missing recommended library: libXi.so Missing recommended library: libXmu.so&#39; solution: sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-devEnvironment Variables: sudo vi ~/.bashrc add following at the end of the file export PATH=/usr/local/cuda-9.0/bin:$PATH export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64 #注意冒号 update configuration sudo source ~/.bashrc Verify if install successfull: cd /usr/local/cuda-9.0/samples/1_Utilities/deviceQuery sudo make ./deviceQuery if succeed, the imformation about GPU will be show on the terminalCheck the version: cat /usr/local/cuda/version.txt Cudnn 7.1.4Home: https://developer.nvidia.com/rdp/cudnn-archivechoose ‘7.1.4 for Linux’download the .tgz FileCmd: tar -zxvf cudnn-9.0-linux-x64-v7.1.tgz cd cuda sudo cp lib64/lib /usr/local/cuda-9.0/lib64/ sudo cp include/cudnn.h /usr/local/cuda-9.0/include/ sudo cp cuda/lib64/libcudnn /usr/local/cuda-9.0/lib64 sudo chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn* update the link cd /usr/local/cuda-9.0/lib64/ sudo chmod +r libcudnn.so.7.1.3 #use ‘ls libcud*’ -&gt; comfirm the version of .so sudo ln -sf libcudnn.so.7.1.3. libcudnn.so.7 sudo ln -sf libcudnn.so.7 libcudnn.so sudo ldconfigCheck the Version: cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 Pytorch 1.1Home: https://pytorch.org/get-started/locally/Select the related versionuse the cmd provided by the web 5.Pycocotools git clone https://github.com/cocodataset/cocoapi.git cd cocoapi/PythonAPI make pip install pycocotools Reference: https://zhuanlan.zhihu.com/p/44183691]]></content>
  </entry>
  <entry>
    <title><![CDATA[Learning Visual N-Grams from Web Data]]></title>
    <url>%2F2019%2F05%2F27%2FLearning-Visual-N-Grams-from-Web-Data%2F</url>
    <content type="text"><![CDATA[a brief introduction to Learning Visual N-Grams from Web Data Summary(1) Dataset: YFCC100M (images-user comments pairs)[30 million](2) Image Feature: ResNet34(3) Loss function: Naive n-gram loss(multi-class logistic loss) &amp;&amp; Jelinek-Mercer (J-M) loss(Jelinek-Mercer smoothing)(4) Experiment setting: SGD | batch-size:128 | epoch: 10 |learning rate: 0.1[learning rate is divided by 10 whenever the training loss stabilizes (until a minimum learning rate of 0.001)](5) Relating Images and Captions: Image retrieval: rank images according to their log-likelihood for a particular caption Caption retrieval: ranking all captions in the test set according to their log-likelihood under the model. Introduction(1) Visual n-gram models: can predict arbitrary phrases that are relevant to the content of an image.(2) Model Structure: feed-forward convolutional networks + new loss functions(inspired by n-gram models commonly used in language modeling)(3) Follow Learning visual features from large weakly supervised data(4) Bridge the semantic gap between vision and language: predict phrases that are relevant to the contents of an image(5) Loss function: optimizes trainable parameters for frequent n-grams, whereas for infrequent n-grams, the loss is dominated by the predicted likelihood of smaller “sub-grams” Learning Visual N-Gram Models3.1 Dataset(1) YFCC100M: 99.2 million images + associated multi-lingual user comments(2) Utilization: 30 million images and English comments(3) Image pretreatment: rescale to 256 X 256 pixels (using bicubic interpolation) -&gt; crop the central 224 x 224 -&gt; subtract the mean pixel value of each image -&gt; divide by the standard deviation of the pixel values.(4) Text pretreatment: remove punctuations -&gt; add [BEGIN] and [END] tokens at the beginning and end of each sentence(5) Building dictionary of all English n-grams(n: 1 ~ 5): more than 1, 000 occurrences in the 30 million English comments. -&gt; 142, 806 n-grams: 22, 869 unigrams, 56, 830 bigrams, 32, 560 trigrams, 17, 351 four-grams, and 13, 196 five-grams 3.2 Loss functions3.2.1 Naive n-gram loss (1) measures the (negative) log-likelihood of in-dictionary n-grams that are present in a comment (2) multi-class logistic loss (3) the loss: the sum of all n-grams that appear in the sentence w &amp;&amp; ignore n-grams that do not appear in the dictionary (4) Problem: cannot do language modeling -&gt; Reason: it does not model a conditional probability-&gt; Solution: an ad-hoc conditional distribution based on the scores produced by the model at prediction time using a “stupid” back-off model (5) Drawbacks: 1) it ignores out-of-dictionary n-grams entirely during training 2) the parameters E that correspond to infrequent in-dictionary words are difficult to pin down 3.2.2 Jelinek-Mercer (J-M) loss Experiments4.3 Relating Images and Captions]]></content>
  </entry>
  <entry>
    <title><![CDATA[Multimodal Convolutional Neural Networks for Matching Image and Sentence]]></title>
    <url>%2F2019%2F05%2F22%2FMultimodal-Convolutional-Neural-Networks-for-Matching-Image-and-Sentence%2F</url>
    <content type="text"><![CDATA[a brief introduction to Multimodal Convolutional Neural Networksfor Matching Image and Sentence Summary(1) m-CNN provides an end-to-end framework with convolutional architectures toexploit image representation, word composition, and the matching relationsbetween the two modalities.(2) Component: image CNN &amp; matching CNN &amp; MLP]]></content>
  </entry>
  <entry>
    <title><![CDATA[Learning Two-Branch Neural Networks for Image-Text Matching Tasks]]></title>
    <url>%2F2019%2F05%2F22%2FLearning-Two-Branch-Neural-Networksfor-Image-Text-Matching-Tasks%2F</url>
    <content type="text"><![CDATA[a brief introduction to Learning Two-Branch Neural Networksfor Image-Text Matching Tasks Summary(1) According to different ways to learn the similarity score, this paperpropose two variants of two-branch networks that follow these two strategies.(2) 2-branch: extract image and text features respectively.(3) Embedding network: map images and text -&gt; explicit joint embedding space(4) Similarity Network: frame image/text correspondence as a binary classification problem.(given a pair, the goal is to output the probability that the two items match.) introduction(1) Tasks: phrase localization &amp; bi-directional image-sentence retrieval(2) Network architecture:extract image/text features -&gt; fuse them -&gt; similarity score(3) Embedding network: a. Goal: map image and text features b. Each branch: data -&gt; nonlinearities(2 layers) -&gt; L2 normalization c. Loss function: bi-directional ranking loss + neighborhood-preserving constraints (4) Similarity network: a.Structure: nonlinearities(2 layers) -&gt; element-wise product-&gt; single vector -&gt; fully connected layers b. Loss function: logistic regression loss c. Weakness(compared to Embedding net): it no longer has an explicitembedding space and cannot encode structural constraints. Embedding and Similarity networks3.1 Overview of Image-Text Tasksembedding network is suitable for image-text retrieval3.2 Embedding networkSGD + Bi-directional ranking loss Neighborhood-preserving constraint + Neighborhood sampling]]></content>
  </entry>
  <entry>
    <title><![CDATA[CM-GANs: Cross-modal Generative Adversarial Networks for Common Representation Learning]]></title>
    <url>%2F2019%2F05%2F20%2FCM-GANs-Cross-modal-Generative-Adversarial-Networksfor-Common-Representation-Learning%2F</url>
    <content type="text"><![CDATA[a brief introduction to CM-GANs: Cross-modal Generative Adversarial Networksfor Common Representation Learning 0.Summary 0.1 Structure (1) Generative Model: VGG19(image) | word CNN(text) -&gt; fully connected layer(first) + batch normalization + RELU -&gt; fully connected layer(second) + batch normalization + RELU + weight-sharing between 2 paths -&gt; Common representations -&gt; softmax layer &amp; decoder Decoder: fully connected layer(first) + batch normalization + RELU -&gt; fully connected layer(second) -&gt; text | image reconstructions (2) Intra-modality Discriminative Model: reconstructions -&gt; fully connected layer -&gt; sigmoid layer original representation: 1 | reconstructed representation: 0 (3) Inter-modality Discriminative Model: a. two-pathway network b. input: common representation + original representation c. each path: softmax layer -&gt; fully connected layer(first) + batch normalization + RELU -&gt; fully connected layer(second) (out: single-value predicted score) -&gt; sigmoid layer d. Label: Image: image common representation: 1 corresponding text representation &amp; mismatched image common representation: 0 Text: text common representation: 1 corresponding image representation &amp; mismatched text common representation: 0 0.2]]></content>
  </entry>
  <entry>
    <title><![CDATA[Stacked Cross Attention for Image-Text Matching]]></title>
    <url>%2F2019%2F05%2F20%2FStacked-Cross-Attention-forImage-Text-Matching%2F</url>
    <content type="text"><![CDATA[a brief introduction to Stacked Cross Attention for Image-Text Matching 0.Summary (1) Image representation: bottom-up attention + Faster-RNN + ResNet101 (2) Sentence represetation: one-hot vector (word -&gt; 300-dimentional vector) =&gt; bi-directional GRU(AVG forward &amp; backword) (3) Common representation: SCA model a. Input: a set of image features(image feature -&gt; a region in an image) + a set of word features (word feature -&gt; a word in a sentence) b. Output: similarity score c. Component: (Image-Text &amp; Text-Image) Stacked Cross Attention (4) Loss function: hinge-based triplet ranking loss(like vsepp) -&gt; finding the hardest negatives based on the similarity (5) Experiments: a.Datasets: COCO | Flickr 30k]]></content>
  </entry>
  <entry>
    <title><![CDATA[Order-Embeddings of Images and Language]]></title>
    <url>%2F2019%2F05%2F17%2FOrder-Embeddings-of-Images-and-Language%2F</url>
    <content type="text"><![CDATA[a brief introduction to Order-Embeddings of Images and Language Abstact(1) Goal: explicitly model -&gt; the partial order structure of this visual-semantic hierarchy(2) method: learn ordered representations Introduction]]></content>
  </entry>
  <entry>
    <title><![CDATA[VSE++: Improving Visual-Semantic Embeddings with Hard Negatives]]></title>
    <url>%2F2019%2F05%2F15%2FVSE-Improving-Visual-Semantic-Embeddings-with-Hard-Negatives%2F</url>
    <content type="text"><![CDATA[a brief introduction to VSE++: Improving Visual-Semantic Embeddings with Hard Negatives Abstract(1) inspired by: hard negative mining(2) propose: a new loss function (a simple change to common loss function)(3) combined with: fine tune + augmented data Introduction(1) basic problem: one of ranking(2) augmented data + fine tune =&gt; improve Caption retrieval(3) the benefit of a more powerful image encoder, with finetuning =&gt; is amplified with the use of the loss function Learning Visual-Semantic Embeddings(1) the target of I-T retrieval: ↑ R@k(2) the Similarity funtion -&gt; is defined on the joint embedding space3.1 Visual-Semantic Embedding (1) Image Encoder: VGG19 | ResNet152 (2) Word Encoder: GRU-based text encoder (3) mappings into the joint embedding space -&gt; linear projections (4) similarity function: inner product (5) cumulative loss over training data: (6) loss function for a single training exemplar: hinge-based triplet ranking loss3.2 Emphasis on Hard Negatives (1) hard negatives: the negatives closest to each training query. (2) new loss function: (to emphasis the hardest negatives) (3) how to find hardest negatives: find them within each mini-batch + random sampling of the mini-batches3.3 Probability of Sampling the Hardest Negative (1) for large enough mini-batchs, with high probability we sample negative captions that are harder than 90% of the entire training set (2) Mini-batches size: 128 Experiments(1) Image: a. previous work: extract image features directly from FC7, the penultimate fully connected layer dimensionality: 4096 for VGG19 | 2048 for ResNet152 b. steps: resize the image to 256 X 256 use either a single crop of size 224 X 224 or the mean of feature vectors for multiple crops of similar size c. 1C | 10C | RC 1C: one center crop 10C: training with 10 crops at fixed locations RC: a single random crop (used in this paper)(2) Caption: a. dimensionality: 300 -&gt; GRU -&gt; 1024(3) Normalization: a. VSE: the caption embedding is normalized, while the image embedding is not. b. VSEPP: normalize both c. reason: Not normalizing the image embedding changes the importance of samples. -&gt; not normalizing the image embedding helped the VSE0 to find a better solution. However, VSE++ is not significantly affected by this normalization. 3.2 Details of Training (1) Adam optimizer (2) at most 30 epochs a. Common: start -&gt; learning rate **0:0002** for 15 epochs -&gt; learning rate to **0:00002** for another 15 epochs b. fine-tuned: trained for 30 epochs with a fixed image encoder -&gt; then learning rate to **0:00002** for another 15 epochs (3) margin: 0.2 (4) solution to over-fitting: choosing the snapshot of the model that performs best on the validation set The best snapshot: selected based on the sum of the recalls on the validation set 3.5 Improving Order Embeddings training set: 10C + RV (1) Order: copied from original paper *Order-Embeddings of Images and Language* (2)Order0: use training settings in orginal paper. a. start training with a learning rate of **0:001** for 15 epochs b. lower the learning rate to **0:0001** for another 15 epochs c. margin: 0:05 d. takes the absolute value of embeddings before computing the similarity function (3)Order++: use the same learning schedule and margin as the experiments in this paper + MH loss function 3.6 Behavior of Loss Functions MH loss depends on a smaller set of triplets compared to the SH loss]]></content>
  </entry>
  <entry>
    <title><![CDATA[Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval]]></title>
    <url>%2F2019%2F04%2F06%2FWebly-Supervised-Joint-Embedding-for-Cross-Modal-Image-Text-Retrieval%2F</url>
    <content type="text"><![CDATA[A brief introduction to the paper Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval Introduction1.1 Cross-modal Retrieval (1) Task: take one type of data as the query to retrieve relevant data of another type. (2) Challenge: how to measure the content similarity between different modalities of data, which is referred as the heterogeneity gap (3)Common Framework: Feature extraction ——— Correlation modal common representation ——— search result ranking and summarization 1.2 Overview of the Proposed Approach (1) VSE (2) a two-stage approach: A. a supervised formulation —&gt; that leverages the available clean image-text pairs from a dataset an aligned representation --&gt; that can be shared across three modalities (e.g., image, tag, text). loss function --&gt; bi-directional ranking loss B. LUPI multitask learning strategies curriculum guided training strategy Related Work2.1 VSE(Visual-Semantic Embedding) (1)2.2 Image-Text Retrieval (1) Methods: CCA | ACMR | VSE | …… (2) Improving aspects: loss function | similarity calculation | Input feartures2.3 Webly supervised learning Approach3.1 Network Structure and Input Feature (1) Network Structure: A. three different branches: expert network + two fully connected embedding layers expert networks —&gt; focus on identifying modality-specific features embedding layers —&gt; convert the modality specific features to modality-robust features (2) Input Feature A. Image: Resnet152 | VGG19 B. Text(Sentence): Word2Vec + GRU C. Tag: Word2Vec (sum / num) 3.2 Train Joint Embedding with Ranking Loss (1) Method: VSE (2) Representation: i’: image feature i : the projection of image feature on the joint space | i = W(i)i’ s’: text embedding s : the projection of text embedding on the joint space | s = W(s)s’ (3) Loss function: (for VSE) | (for VSE++) s-: non-matching text embedding for image embedding i s : matching text embedding for image embedding i (i &amp; i- are similar to s &amp; s-) ∆ : margin value f(i,s)； scoring function， measuring the similarity between the images and text in the joint embedded space.(cosine similarity used in this work)3.3 Training Joint Embedding with Web Data (1) Obstacle: A. GRU based approach is not suitable for representing tags since tags do not have any semantic context as in the sentences. -&gt; it is not possible to directly update the embedding using image-tag pairs. B. training data do not provide three modality information at the same time -&gt; employing LUPI strategies are also not possible (2) Two-stage approach 1) Training initial Joint Embedding A. consider nouns and verbs from relevant sentence as dummy tags for an image B. combine the image-text ranking loss objective with image-tag ranking loss objective. 2) Model Adaptation with Web Data A. a smaller learning rate B. a curriculum learning-based strategy: in the early stages of training, the network is presented with images related to frequently occurring concepts/keywords in the clean training set. Datasets and Evaluation Metric4.1 Datasets: MSCOCO | Flickr30K | Web Image Collection]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
