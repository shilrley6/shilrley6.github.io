<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Multimodal Convolutional Neural Networks for Matching Image and Sentence]]></title>
    <url>%2F2019%2F05%2F22%2FMultimodal-Convolutional-Neural-Networks-for-Matching-Image-and-Sentence%2F</url>
    <content type="text"><![CDATA[a brief introduction to Multimodal Convolutional Neural Networksfor Matching Image and Sentence Summary(1) m-CNN provides an end-to-end framework with convolutional architectures toexploit image representation, word composition, and the matching relationsbetween the two modalities.(2) Component: image CNN &amp; matching CNN &amp; MLP]]></content>
  </entry>
  <entry>
    <title><![CDATA[Learning Two-Branch Neural Networks for Image-Text Matching Tasks]]></title>
    <url>%2F2019%2F05%2F22%2FLearning-Two-Branch-Neural-Networksfor-Image-Text-Matching-Tasks%2F</url>
    <content type="text"><![CDATA[a brief introduction to Learning Two-Branch Neural Networksfor Image-Text Matching Tasks Summary(1) According to different ways to learn the similarity score, this paperpropose two variants of two-branch networks that follow these two strategies.(2) 2-branch: extract image and text features respectively.(3) Embedding network: map images and text -&gt; explicit joint embedding space(4) Similarity Network: frame image/text correspondence as a binary classification problem.(given a pair, the goal is to output the probability that the two items match.) introduction(1) Tasks: phrase localization &amp; bi-directional image-sentence retrieval(2) Network architecture:extract image/text features -&gt; fuse them -&gt; similarity score(3) Embedding network: a. Goal: map image and text features b. Each branch: data -&gt; nonlinearities(2 layers) -&gt; L2 normalization c. Loss function: bi-directional ranking loss + neighborhood-preserving constraints (4) Similarity network: a.Structure: nonlinearities(2 layers) -&gt; element-wise product-&gt; single vector -&gt; fully connected layers b. Loss function: logistic regression loss c. Weakness(compared to Embedding net): it no longer has an explicitembedding space and cannot encode structural constraints. Embedding and Similarity networks3.1 Overview of Image-Text Tasksembedding network is suitable for image-text retrieval3.2 Embedding networkSGD + Bi-directional ranking loss Neighborhood-preserving constraint + Neighborhood sampling]]></content>
  </entry>
  <entry>
    <title><![CDATA[CM-GANs: Cross-modal Generative Adversarial Networks for Common Representation Learning]]></title>
    <url>%2F2019%2F05%2F20%2FCM-GANs-Cross-modal-Generative-Adversarial-Networksfor-Common-Representation-Learning%2F</url>
    <content type="text"><![CDATA[a brief introduction to CM-GANs: Cross-modal Generative Adversarial Networksfor Common Representation Learning 0.Summary 0.1 Structure (1) Generative Model: VGG19(image) | word CNN(text) -&gt; fully connected layer(first) + batch normalization + RELU -&gt; fully connected layer(second) + batch normalization + RELU + weight-sharing between 2 paths -&gt; Common representations -&gt; softmax layer &amp; decoder Decoder: fully connected layer(first) + batch normalization + RELU -&gt; fully connected layer(second) -&gt; text | image reconstructions (2) Intra-modality Discriminative Model: reconstructions -&gt; fully connected layer -&gt; sigmoid layer original representation: 1 | reconstructed representation: 0 (3) Inter-modality Discriminative Model: a. two-pathway network b. input: common representation + original representation c. each path: softmax layer -&gt; fully connected layer(first) + batch normalization + RELU -&gt; fully connected layer(second) (out: single-value predicted score) -&gt; sigmoid layer d. Label: Image: image common representation: 1 corresponding text representation &amp; mismatched image common representation: 0 Text: text common representation: 1 corresponding image representation &amp; mismatched text common representation: 0 0.2]]></content>
  </entry>
  <entry>
    <title><![CDATA[Stacked Cross Attention for Image-Text Matching]]></title>
    <url>%2F2019%2F05%2F20%2FStacked-Cross-Attention-forImage-Text-Matching%2F</url>
    <content type="text"><![CDATA[a brief introduction to Stacked Cross Attention for Image-Text Matching 0.Summary (1) Image representation: bottom-up attention + Faster-RNN + ResNet101 (2) Sentence represetation: one-hot vector (word -&gt; 300-dimentional vector) =&gt; bi-directional GRU(AVG forward &amp; backword) (3) Common representation: SCA model a. Input: a set of image features(image feature -&gt; a region in an image) + a set of word features (word feature -&gt; a word in a sentence) b. Output: similarity score c. Component: (Image-Text &amp; Text-Image) Stacked Cross Attention (4) Loss function: hinge-based triplet ranking loss(like vsepp) -&gt; finding the hardest negatives based on the similarity (5) Experiments: a.Datasets: COCO | Flickr 30k]]></content>
  </entry>
  <entry>
    <title><![CDATA[Order-Embeddings of Images and Language]]></title>
    <url>%2F2019%2F05%2F17%2FOrder-Embeddings-of-Images-and-Language%2F</url>
    <content type="text"><![CDATA[a brief introduction to Order-Embeddings of Images and Language Abstact(1) Goal: explicitly model -&gt; the partial order structure of this visual-semantic hierarchy(2) method: learn ordered representations Introduction]]></content>
  </entry>
  <entry>
    <title><![CDATA[VSE++: Improving Visual-Semantic Embeddings with Hard Negatives]]></title>
    <url>%2F2019%2F05%2F15%2FVSE-Improving-Visual-Semantic-Embeddings-with-Hard-Negatives%2F</url>
    <content type="text"><![CDATA[a brief introduction to VSE++: Improving Visual-Semantic Embeddings with Hard Negatives Abstract(1) inspired by: hard negative mining(2) propose: a new loss function (a simple change to common loss function)(3) combined with: fine tune + augmented data Introduction(1) basic problem: one of ranking(2) augmented data + fine tune =&gt; improve Caption retrieval(3) the benefit of a more powerful image encoder, with finetuning =&gt; is amplified with the use of the loss function Learning Visual-Semantic Embeddings(1) the target of I-T retrieval: ↑ R@k(2) the Similarity funtion -&gt; is defined on the joint embedding space3.1 Visual-Semantic Embedding (1) Image Encoder: VGG19 | ResNet152 (2) Word Encoder: GRU-based text encoder (3) mappings into the joint embedding space -&gt; linear projections (4) similarity function: inner product (5) cumulative loss over training data: (6) loss function for a single training exemplar: hinge-based triplet ranking loss3.2 Emphasis on Hard Negatives (1) hard negatives: the negatives closest to each training query. (2) new loss function: (to emphasis the hardest negatives) (3) how to find hardest negatives: find them within each mini-batch + random sampling of the mini-batches3.3 Probability of Sampling the Hardest Negative (1) for large enough mini-batchs, with high probability we sample negative captions that are harder than 90% of the entire training set (2) Mini-batches size: 128 Experiments(1) Image: a. previous work: extract image features directly from FC7, the penultimate fully connected layer dimensionality: 4096 for VGG19 | 2048 for ResNet152 b. steps: resize the image to 256 X 256 use either a single crop of size 224 X 224 or the mean of feature vectors for multiple crops of similar size c. 1C | 10C | RC 1C: one center crop 10C: training with 10 crops at fixed locations RC: a single random crop (used in this paper)(2) Caption: a. dimensionality: 300 -&gt; GRU -&gt; 1024(3) Normalization: a. VSE: the caption embedding is normalized, while the image embedding is not. b. VSEPP: normalize both c. reason: Not normalizing the image embedding changes the importance of samples. -&gt; not normalizing the image embedding helped the VSE0 to find a better solution. However, VSE++ is not significantly affected by this normalization. 3.2 Details of Training (1) Adam optimizer (2) at most 30 epochs a. Common: start -&gt; learning rate **0:0002** for 15 epochs -&gt; learning rate to **0:00002** for another 15 epochs b. fine-tuned: trained for 30 epochs with a fixed image encoder -&gt; then learning rate to **0:00002** for another 15 epochs (3) margin: 0.2 (4) solution to over-fitting: choosing the snapshot of the model that performs best on the validation set The best snapshot: selected based on the sum of the recalls on the validation set 3.5 Improving Order Embeddings training set: 10C + RV (1) Order: copied from original paper *Order-Embeddings of Images and Language* (2)Order0: use training settings in orginal paper. a. start training with a learning rate of **0:001** for 15 epochs b. lower the learning rate to **0:0001** for another 15 epochs c. margin: 0:05 d. takes the absolute value of embeddings before computing the similarity function (3)Order++: use the same learning schedule and margin as the experiments in this paper + MH loss function 3.6 Behavior of Loss Functions MH loss depends on a smaller set of triplets compared to the SH loss]]></content>
  </entry>
  <entry>
    <title><![CDATA[Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval]]></title>
    <url>%2F2019%2F04%2F06%2FWebly-Supervised-Joint-Embedding-for-Cross-Modal-Image-Text-Retrieval%2F</url>
    <content type="text"><![CDATA[A brief introduction to the paper Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval Introduction1.1 Cross-modal Retrieval (1) Task: take one type of data as the query to retrieve relevant data of another type. (2) Challenge: how to measure the content similarity between different modalities of data, which is referred as the heterogeneity gap (3)Common Framework: Feature extraction ——— Correlation modal common representation ——— search result ranking and summarization 1.2 Overview of the Proposed Approach (1) VSE (2) a two-stage approach: A. a supervised formulation —&gt; that leverages the available clean image-text pairs from a dataset an aligned representation --&gt; that can be shared across three modalities (e.g., image, tag, text). loss function --&gt; bi-directional ranking loss B. LUPI multitask learning strategies curriculum guided training strategy Related Work2.1 VSE(Visual-Semantic Embedding) (1)2.2 Image-Text Retrieval (1) Methods: CCA | ACMR | VSE | …… (2) Improving aspects: loss function | similarity calculation | Input feartures2.3 Webly supervised learning Approach3.1 Network Structure and Input Feature (1) Network Structure: A. three different branches: expert network + two fully connected embedding layers expert networks —&gt; focus on identifying modality-specific features embedding layers —&gt; convert the modality specific features to modality-robust features (2) Input Feature A. Image: Resnet152 | VGG19 B. Text(Sentence): Word2Vec + GRU C. Tag: Word2Vec (sum / num) 3.2 Train Joint Embedding with Ranking Loss (1) Method: VSE (2) Representation: i’: image feature i : the projection of image feature on the joint space | i = W(i)i’ s’: text embedding s : the projection of text embedding on the joint space | s = W(s)s’ (3) Loss function: (for VSE) | (for VSE++) s-: non-matching text embedding for image embedding i s : matching text embedding for image embedding i (i &amp; i- are similar to s &amp; s-) ∆ : margin value f(i,s)； scoring function， measuring the similarity between the images and text in the joint embedded space.(cosine similarity used in this work)3.3 Training Joint Embedding with Web Data (1) Obstacle: A. GRU based approach is not suitable for representing tags since tags do not have any semantic context as in the sentences. -&gt; it is not possible to directly update the embedding using image-tag pairs. B. training data do not provide three modality information at the same time -&gt; employing LUPI strategies are also not possible (2) Two-stage approach 1) Training initial Joint Embedding A. consider nouns and verbs from relevant sentence as dummy tags for an image B. combine the image-text ranking loss objective with image-tag ranking loss objective. 2) Model Adaptation with Web Data A. a smaller learning rate B. a curriculum learning-based strategy: in the early stages of training, the network is presented with images related to frequently occurring concepts/keywords in the clean training set. Datasets and Evaluation Metric4.1 Datasets: MSCOCO | Flickr30K | Web Image Collection]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
