<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval]]></title>
    <url>%2F2019%2F04%2F06%2FWebly-Supervised-Joint-Embedding-for-Cross-Modal-Image-Text-Retrieval%2F</url>
    <content type="text"><![CDATA[A brief introduction to the paper &lt;&gt; Introduction1.1 Cross-modal Retrieval (1) Task: take one type of data as the query to retrieve relevant data of another type. (2) Challenge: how to measure the content similarity between different modalities of data, which is referred as the heterogeneity gap (3)Common Framework: Feature extraction ——— Correlation modal common representation ——— search result ranking and summarization 1.2 Overview of the Proposed Approach (1) VSE (2) a two-stage approach: A. a supervised formulation —&gt; that leverages the available clean image-text pairs from a dataset an aligned representation --&gt; that can be shared across three modalities (e.g., image, tag, text). loss function --&gt; bi-directional ranking loss B. LUPI multitask learning strategies curriculum guided training strategy Related Work2.1 VSE(Visual-Semantic Embedding) (1)2.2 Image-Text Retrieval (1) Methods: CCA | ACMR | VSE | …… (2) Improving aspects: loss function | similarity calculation | Input feartures2.3 Webly supervised learning Approach3.1 Network Structure and Input Feature (1) Network Structure: A. three different branches: expert network + two fully connected embedding layers expert networks —&gt; focus on identifying modality-specific features embedding layers —&gt; convert the modality specific features to modality-robust features (2) Input Feature A. Image: Resnet152 | VGG19 B. Text(Sentence): Word2Vec + GRU C. Tag: Word2Vec (sum / num) 3.2 Train Joint Embedding with Ranking Loss (1) Method: VSE (2) Representation: i’: image feature i : the projection of image feature on the joint space | i = W(i)i’ s’: text embedding s : the projection of text embedding on the joint space | s = W(s)s’ (3) Loss function: (for VSE) | (for VSE++) s-: non-matching text embedding for image embedding i s : matching text embedding for image embedding i (i &amp; i- are similar to s &amp; s-) ∆ : margin value f(i,s)； scoring function， measuring the similarity between the images and text in the joint embedded space.(cosine similarity used in this work)3.3 Training Joint Embedding with Web Data (1) Obstacle: A. GRU based approach is not suitable for representing tags since tags do not have any semantic context as in the sentences. -&gt; it is not possible to directly update the embedding using image-tag pairs. B. training data do not provide three modality information at the same time -&gt; employing LUPI strategies are also not possible (2) Two-stage approach 1) Training initial Joint Embedding A. consider nouns and verbs from relevant sentence as dummy tags for an image B. combine the image-text ranking loss objective with image-tag ranking loss objective. 2) Model Adaptation with Web Data A. a smaller learning rate B. a curriculum learning-based strategy: in the early stages of training, the network is presented with images related to frequently occurring concepts/keywords in the clean training set. Datasets and Evaluation Metric4.1 Datasets: MSCOCO | Flickr30K | Web Image Collection]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
